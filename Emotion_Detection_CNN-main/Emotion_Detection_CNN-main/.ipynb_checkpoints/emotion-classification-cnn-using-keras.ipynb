{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam,SGD,RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение нейросети, задавание генераторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 128\n",
    "\n",
    "datagen_train  = ImageDataGenerator()\n",
    "datagen_val = ImageDataGenerator()\n",
    "\n",
    "train_set = datagen_train.flow_from_directory('C:/Emotion_Detection_CNN-main/Emotion_Detection_CNN-main/Dataset/train',\n",
    "                                              target_size = (48,48),\n",
    "                                              color_mode = \"grayscale\",\n",
    "                                              batch_size=64,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "\n",
    "\n",
    "test_set = datagen_val.flow_from_directory('C:/Emotion_Detection_CNN-main/Emotion_Detection_CNN-main/Dataset/train',\n",
    "                                              target_size = (48,48),\n",
    "                                              color_mode = \"grayscale\",\n",
    "                                              batch_size=64,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 512)       590336    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 12, 12, 512)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1179904   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,478,727\n",
      "Trainable params: 4,474,759\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "\n",
    "no_of_classes = 7\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#1-й слой сверточной нейросети\n",
    "model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#2-й слой сверточной нейросети\n",
    "model.add(Conv2D(128,(5,5),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.25))\n",
    "\n",
    "#3-й слой сверточной нейросети\n",
    "model.add(Conv2D(512,(3,3),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.25))\n",
    "\n",
    "#4-й слой нейросети\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Fully connected 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(no_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подгонка модели к данным обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "epochs = 48\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Efim\\AppData\\Local\\Temp\\ipykernel_10232\\673870691.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(generator=train_set,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n",
      "  3/448 [..............................] - ETA: 13:10 - loss: 2.3871 - accuracy: 0.1667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYello\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\keras\\engine\\training.py:2604\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2593\u001b[0m \n\u001b[0;32m   2594\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2595\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2596\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2598\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2599\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2600\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2601\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2602\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2603\u001b[0m )\n\u001b[1;32m-> 2604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2606\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\Emotion_Detection_CNN-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_set,\n",
    "                                steps_per_epoch=train_set.n//train_set.batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = test_set,\n",
    "                                validation_steps = test_set.n//test_set.batch_size,\n",
    "                                callbacks=callbacks_list\n",
    "                                )\n",
    "print('Yello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Accuracy & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizer : Adam\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper right\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('dark_background')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
